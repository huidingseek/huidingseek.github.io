<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hui Ding</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="images/huiding.jpg"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hui Ding, PhD</name>
              </p>
              <p>
                <!-- I am an Applied Research Scientist in the Generative AI team at  <a href="https://www.amd.com/en.html">AMD</a>. I received my Ph.D degree from <a href="https://engineering.jhu.edu/ece/">Department of Electrical and Computer Engineering</a>, <a href="https://www.jhu.edu/">Johns Hopkins University</a> in 2024,
                advised by <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Prof. Rama Chellappa</a>. I was awarded the <a href="https://www.amazon.science/news-and-features/johns-hopkins-and-amazon-announce-four-fellow-and-eight-faculty-research-awards">Amazon AI2AI Fellowship</a> in 2023.
                I received my BSE degree from <a href="https://www.au.tsinghua.edu.cn/">Department of Automation</a>, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> in 2019 advised by <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/">Prof. Jianjiang Feng</a>
                and <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>. -->

                I am a Senior Applied Scientist at Adobe <a href="https://www.adobe.com/products/firefly.html">Firefly</a>. I work on foundation models for generative AI including auto-regressive modeling for vision-language multi-modality learning and text-to-image/video generation and editing.
              </p>
              <p>
                Before that I worked at Amazon <a href="https://amazon.jobs/content/en/teams/agi">AGI Foundations</a> and <a href="https://aws.amazon.com/ai/">AWS AI Labs</a> where I was part of the team that launched <a href="https://aws.amazon.com/ai/generative-ai/nova/">Titan Image Generator (Nova Canvas)</a>. 
                I received my PhD at <a href="https://www.umd.edu/">University of Maryland, College Park</a> in 2020, advised by Professor <a href="https://scholar.google.com/citations?user=L60tuywAAAAJ&hl=en">Rama Chellappa</a>. 
                <!-- I received my Master degree from Shanghai Jiao Tong University and Bachelor degree from Dalian University of Technology.  -->
                I interned at <a href="https://waymo.com/">Waymo</a>, <a href="https://research.adobe.com/">Adobe Research</a>, <a href="https://www.parc.com/">Palo Alto Research Center</a>, and <a href="https://www.siemens.com/healthineers">Siemens Healthineers</a> during my PhD. 
                


              </p>
              <!-- <p>
                In summer 2023, I interned at <a href="https://azure.microsoft.com/en-us/solutions/ai"> Microsoft Azure AI </a> with <a href="https://jianfengwang.me/">Dr. Jianfeng Wang</a> working on multi-modal large language models. In summer 2022, I worked as an Applied Scientist Intern at <a href="https://aws.amazon.com/machine-learning/ai-services/"> Amazon AWS AI </a> working on vision-language models mentored by
                <a href="http://www.huiding.org">Dr. Hui Ding</a>,
                <a href="https://zhaoweicai.github.io/">Dr. Zhaowei Cai</a>, and
                <a href="https://www.ytzhang.net/">Dr. Yuting Zhang</a>. I've also worked as a Deep Learning Research Scientist Intern
                at <a href="https://subtlemedical.com/">Subtle Medical</a> developing novel Transformer-based magnetic resonance imaging (MRI) algorithms.

              </p> -->

              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=btla44EAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/hui-ding-373215a3/">LinkedIn</a> &nbsp/&nbsp
                <a href="mailto:huiding@adobe.com">Email</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/huiding.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/huiding_crop.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My current research interests include large language models, vision-language models, and trustworthy AI.
                <p style="color:red;">We are hiring research interns in all areas of generative AI! Feel free to drop me an email with your CV if interested. </p>
              </p>
            </td>
          </tr> -->
        </tbody></table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <tr onmouseout="i2a_stop()" onmouseover="i2a_start()" style="margin: 0; padding: 0; line-height: 1.2;">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="i2a_image" style="opacity: 1;">
                <img src="./images/teaser.png" width="200"></div>
              <img src="./images/Pipeline.png" width="200">
            </div>
            <script type="text/javascript">
              function i2a_start() {
                document.getElementById('i2a_image').style.opacity = "0";
              }

              function i2a_stop() {
                document.getElementById('i2a_image').style.opacity = "1";
              }
              i2a_stop()
            </script>
          </td>
          <td style="padding:40px;width:100%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2412.07774">
              <papertitle> UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics
              </papertitle>
            </a>
            <br>
            Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, <strong>Hui Ding</strong>, Zhe Lin, Hengshuang Zhao
            <br>
            <em>CVPR Highlight</em>, 2025
            <br>
            <a href="https://xavierchen34.github.io/UniReal-Page/">Project Page</a> /
            <a href="https://arxiv.org/abs/2412.07774">arXiv</a>
            <br>
            <p></p>
            <p>
              As a universal framework, UniReal supports a broad spectrum of image generation and editing tasks within a single model, accommodating diverse input-output configurations and generating highly realistic results, which effectively handle challenging scenarios, e.g., shadows, reflections, lighting effects, object pose changes, etc. </p>
          </td>
        </tr>
        <tr onmouseout="poly_stop()" onmouseover="poly_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="poly_image" style="opacity: 1;">
                <img src="./images/poly_1.png" width="200"></div>
              <img src="./images/poly_2.png" width="200">
            </div>
            <script type="text/javascript">
              function poly_start() {
                document.getElementById('poly_image').style.opacity = "0";
              }

              function poly_stop() {
                document.getElementById('poly_image').style.opacity = "1";
              }
              sac_stop()
            </script>
          </td>
          <td style="padding:40px;width:100%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2302.07387">
              <papertitle> PolyFormer: Referring Image Segmentation as Sequential Polygon Generation
              </papertitle>
            </a>
            <br>
            <strong>Jiang Liu*</strong>, Hui Ding*, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, R. Manmatha (*equal contribution)
            <br>
            <em>CVPR</em>, 2023
            <br>
            <a href="https://polyformer.github.io/">Project Page</a> /
            <a href="https://arxiv.org/abs/2302.07387">arXiv</a> /
             <a href="https://github.com/amazon-science/polygon-transformer">code</a> 
            <!-- <a href="bibs/Liu_2023_CVPR.bib">bibtex</a> -->
            <br>
            <p></p>
            <p>
              Instead of directly predicting the pixel-level segmentation masks, the problem of referring image segmentation is formulated as sequential polygon generation, and the predicted polygons can be later converted into segmentation masks. This is enabled by a new sequence-to-sequence framework, Polygon Transformer (PolyFormer), which takes a sequence of image patches and text query tokens as input, and outputs a sequence of polygon vertices autoregressively. 
          </td>
        </tr>
        <tr onmouseout="dp_stop()" onmouseover="dp_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="dp_image" style="opacity: 1;">
                <img src="./images/deepfake_teaser.png" width="200"></div>
              <img src="./images/deepfake_pipeline.png" width="200">
            </div>
            <script type="text/javascript">
              function dp_start() {
                document.getElementById('dp_image').style.opacity = "0";
              }

              function dp_stop() {
                document.getElementById('dp_image').style.opacity = "1";
              }
              dp_stop()
            </script>
          </td>
          <td style="padding:40px;width:100%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2012.09311">
              <papertitle> Learning Self-Consistency for Deepfake Detection
              </papertitle>
            </a>
            <br>
            Tianchen Zhao, Xiang Xu, Mingze Xu, <strong>Hui Ding</strong>, Yuanjun Xiong, and Wei Xia
            <br>
            <em>ICCV Oral</em>, 2021
            <br>
            <a href="https://arxiv.org/pdf/2012.09311">arXiv</a> 
            <!-- <a href="bibs/liu_2023_diffprotect.bib">bibtex</a> / -->
            <!-- <a href="https://github.com/joellliu/DiffProtect">code</a> -->
            <br>
            <p></p>
            <p>
              We introduce a novel representation learning ap-
proach, called pair-wise self-consistency learning (PCL),
for training ConvNets to extract these source features and
detect deepfake images. 
</p>
            </td>
        </tr>


          <tr onmouseout="ijsat_stop()" onmouseover="ijsat_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="ijsat_image" style="opacity: 1;">
                <img src="./images/ijcb_pipeline.png" width="200"></div>
              <img src="./images/ijcb_pipeline.png" width="200">
            </div>
            <script type="text/javascript">
              function ijsat_start() {
                document.getElementById('ijsat_image').style.opacity = "0";
              }

              function ijsat_stop() {
                document.getElementById('ijsat_image').style.opacity = "1";
              }
              ijsat_stop()
            </script>
          </td>
          <td style="padding:40px;width:100%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2005.06040">
              <papertitle> Occlusion-Adaptive Deep Network for Robust Facial Expression Recognition
              </papertitle>
            </a>
            <br>
            <strong>Hui Ding</strong>, Peng Zhou and Rama Chellappa
            <br>
            <em>International Joint Conference on Biometrics (IJCB) Oral</em>, 2020
            <br>
            <!-- <a href="https://ieeexplore.ieee.org/abstract/document/10155464">IEEE</a> / -->
            <a href="https://arxiv.org/pdf/2005.06040">arXiv</a> 
            <!-- <a href="bibs/lau_2021_ijsat.bib">bibtex</a> -->
            <br>
            <p></p>
            <p>
              We propose a landmark-guided attention branch to find and discard corrupted features from occluded regions so that they are not used for recognition. 
              To further improve robustness, we propose a facial region branch to partition the feature maps into non-overlapping facial blocks 
              and task each block to predict the expression independently. </p>
          </td>
        </tr>



          <tr onmouseout="mmt_stop()" onmouseover="mmt_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="mmt_image" style="opacity: 1;">
                <img src="./images/exprgan_teaser.png" width="200"></div>
              <img src="./images/exprgan_pipeline.png" width="150">
            </div>
            <script type="text/javascript">
              function mmt_start() {
                document.getElementById('mmt_image').style.opacity = "0";
              }

              function mmt_stop() {
                document.getElementById('mmt_image').style.opacity = "1";
              }
              mmt_stop()
            </script>
          </td>
          <td style="padding:40px;width:100%;vertical-align:middle">
            <a href="https://arxiv.org/abs/1709.03842">
              <papertitle> ExprGAN: Facial Expression Editing with Controllable Expression Intensity
              </papertitle>
            </a>
            <br>
            <strong>Hui Ding</strong>, Kumar Sricharan and Rama Chellappa
            <br>
            <em>AAAI Oral</em>, 2018
            <br>
            <a href="https://arxiv.org/abs/1709.03842">arXiv</a> /
            <a href="https://github.com/huiding/ExprGAN">code</a>
            <br>
            <p></p>
            <p>
              We propose an Expression Generative Adversarial Network (ExprGAN) for photo-realistic facial expression editing with controllable expression intensity. An expression controller module is specially designed to learn an expressive and compact expression code in addition to the encoder-decoder network.
              This novel architecture
enables the expression intensity to be continuously adjusted
from low to high.
          </td>
        </tr>
        </tr>
         <tr onmouseout="sac_stop()" onmouseover="sac_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="sac_image" style="opacity: 1;">
                <img src="./images/cascade_teaser.png" width="200"></div>
              <img src="./images/cascade_pipeline.png" width="200">
            </div>
            <script type="text/javascript">
              function sac_start() {
                document.getElementById('sac_image').style.opacity = "0";
              }

              function sac_stop() {
                document.getElementById('sac_image').style.opacity = "1";
              }
              sac_stop()
            </script>
          </td>
          <td style="padding:40px;width:100%;vertical-align:middle">
            <a href="https://arxiv.org/abs/1709.03851">
              <papertitle> A Deep Cascade Network for Unaligned Face Attribute Classification
              </papertitle>
            </a>
            <br>
            <strong>Hui Ding</strong>, Hao Zhou, Shaohua Kevin Zhou and Rama Chellappa
            <br>
            <em>AAAI Spotlight</em>, 2018
            <br>
            <!-- <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Segment_and_Complete_Defending_Object_Detectors_Against_Adversarial_Patch_Attacks_CVPR_2022_paper.pdf">PDF</a> / -->
            <!-- <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Liu_Segment_and_Complete_CVPR_2022_supplemental.pdf">Supp</a> / -->
            <a href="https://arxiv.org/abs/1709.03851">arXiv</a> 
            <!-- <a href="bibs/Liu_2022_CVPR.bib">bibtex</a> / -->
            <!-- <a href="https://github.com/joellliu/SegmentAndComplete">code</a> / -->
            <!-- <a href="https://aiem.jhu.edu/datasets/apricot-mask">Apricot-Mask Dataset</a> -->
            <!-- <a href="https://github.com/hsouri/Sleeper-Agent">code</a> -->
<!--             <a href="https://github.com/hsouri/hsouri.github.io/blob/master/Data/khorramshahi2020gans.bib">bibtex</a> -->
            <br>
            <p></p>
            <p>
              We propose a cascade network
that simultaneously learns to localize face regions specific to
attributes and performs attribute classification without alignment. 
First, a weakly-supervised face region localization net-
work is designed to automatically detect regions (or parts)
specific to attributes. Then multiple part-based networks and
a whole-image-based network are separately constructed and
combined together by the region switch layer and attribute re-
lation layer for final attribute classification. 
            </p>
          </td>
        </tr>

        <tr onmouseout="mat_stop()" onmouseover="mat_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="mat_image" style="opacity: 1;">
                <img src="./images/expnet_teaser.png" width="200"></div>
              <img src="./images/expnet_pipeline.png" width="200">
            </div>
            <script type="text/javascript">
              function mat_start() {
                document.getElementById('mat_image').style.opacity = "0";
              }

              function mat_stop() {
                document.getElementById('mat_image').style.opacity = "1";
              }
              mat_stop()
            </script>
          </td>
          <td style="padding:40px;width:100%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/1609.06591v1">
              <papertitle> Facenet2expnet: Regularizing a deep face recognition net for expression recognition
              </papertitle>
            </a>
            <br>
            <strong>Hui Ding</strong>, Shaohua Kevin Zhou and Rama Chellappa
            <br>
            <em>IEEE International Conference on Automatic Face Gesture Recognition (FG)</em>, 2017
            <br>
            <!-- <a href="https://ieeexplore.ieee.org/document/9798870">IEEE</a> / -->
            <a href="https://arxiv.org/pdf/1609.06591v1">arXiv</a> 
            <!-- <a href="bibs/Liu_2022_mutual.bib">bibtex</a> -->
            <br>
            <p></p>
            <p>
              Relatively small data sets available for expression
recognition research make the training of deep networks for
expression recognition very challenging. We present FaceNet2ExpNet, a novel idea to train an expression
recognition network based on static images. We first propose
a new distribution function to model the high-level neurons
of the expression network. Based on this, a two-stage training
algorithm is carefully designed.
            </p>
          </td>
        </tr>


						



					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">

                <!-- Source code credit to  <a href="https://jonbarron.info/">Dr. Jon Barron</a>. -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
